<html>
<head>
	<title>Results</title>
    <link rel="stylesheet" type="text/css" href="../static/css/styles.css">
</head>
<body>
	<header>
		<div class="container">
			<h2>Spam Detector For SMS Messages</h2>
		  <h3>A Machine Learning App That Explains It All</h3>
		</div>
	</header>
	<div class="results">
		{% if prediction == 1%}
			<h2 style="color:#e0392d;">That was not a nice message...</h2>
		{% elif prediction == 0%}
			<h2 style="color:#3d7aba;">Thank you, that <i>was</i> a nice message.</h2>
		{% endif %}
	</div>
	<p>
		<h3>How did the machine learning algorithm arrive at this prediction from your text message?</h3>
		<br>
		At a high level, these are the main steps in this app:
		<br><br>
		<center><img src="../static/img/Explanation1.PNG" alt="Explanation 1" width=95%></center>
		<p style="font-size:10px" align="center">
			Interior of a classroom, students bent at study, Yallourn, 1947.
			<br>
 			Photo by <a href="https://unsplash.com/@museumsvictoria">Museums Victoria</a>
			in <a href="https://unsplash.com/">Unsplash</a>.
		</p>
		<br>
		<h2 style="color:#3c90b5; text-align:center">Processing The Text</h2>
		Most of the time is spent processing the text as this numeric array that the ML model can understand.
		Prediction happens quickly after that. Processing takes several steps:
		<ul>
			<li>Creating Bag of Words (BoW)</li>
			<li>Tranforming the BoW into a Document-Term Frequency Matrix (DTM)</li>
			<li>Converting the DTM into a Document-Term TF-IDF Matrix</li>
			<li>Reducing Dimensions via Singular Value Decomposition</li>
			<li>Computing Mean Spam Cosine Similarities</li>
		</ul>
		<br>
		As a motivating example, let's follow this spam text taken from our collection (with some modifications):
		<p style="font-family:monospace; text-align:center">
			For a chance to win a å£250 cash TXT: ACTION to 80608. U won't be sorry -visits @www.movietrivia.tv @8pm PDT!
		</p>
		<br>
		<h3>Creating a Bag of Words (BoW)</h3>
		<br>
		<b>Parsing:</b> we cleanup the text, that is: lower case, remove  punctuation, replace URLs and numbers, and so forth.
		<br><br>
		<button onclick="ShowDeets('Deets1')">See Details</button>
		<br>
		<div id="Deets1">
		<br>
		These are some of the steps performed, for a deeper look into all the preprocessing see the source code (link below).
		<br><br>
			<table class="table table-striped">
				 <thead>
			 		 <tr>
						 	<th>Step</th>
							<th>Result</th>
					</tr>
				 </thead>
				 <tbody>
						 	<tr>
								<td>Expand Contractions</td>
								<td>For a chance to win a å£250 cash TXT: ACTION to 80608. U will be sorry -visits @www.movietrivia.tv @8pm PDT!</td>
							</tr>
							<tr>
								<td>Lower Case</td>
								<td>for a chance to win a å£250 cash txt: action to 80608. u will not be sorry -visits @www.movietrivia.tv @8pm pdt!</td>
							</tr>
							<tr>
								<td>Replace URLs</td>
								<td>for a chance to win a å£250 cash txt: action to 80608. u will not be sorry -visits  URL  @8pm pdt!</td>
							</tr>
							<tr>
								<td>Replace Numbers</td>
								<td>for a chance to win a å£ NUM  cash txt: action to  NUM . u will not be sorry -visits  URL  @ NUM pm pdt!</td>
							</tr>
							<tr>
								<td>Remove Punctuation</td>
								<td>for a chance to win a å NUM cash txt action to NUM u will not be sorry visits URL NUM pm pdt</td>
							</tr>
							<tr>
								<td>Replace Emojis</td>
								<td>for a chance to win a  EMOJI  NUM cash txt action to NUM u will not be sorry visits URL NUM pm pdt</td>
							</tr>
				 </tbody>
			</table>
		</div>
		<br>
		<b>Tokenizing:</b> we split the text into chunks (tokens), remove common words, lemmatize, and group into ngrams.
		<br>
		<br>
		<button onclick="ShowDeets('Deets2')">See Details</button>
		<div id="Deets2">
		<br>
		Very common words ("stop words") are removed as they provide little signal, and words are
		<a href="https://en.wikipedia.org/wiki/Lemmatisation">lemmatized</a> so as to standardize some of the variety and try to capture more signal.
		We then form two and three-long sequences of tokens (bigrams and trigrams) to capture some of the order in the text.
		These steps attempt to capture some of the meaning behind a message. There are many other ways to do this, for example, with deep learning architectures
		and <a href="https://en.wikipedia.org/wiki/Long_short-term_memory">LSTM (Long-Short-Term-Memory)</a> cells I could've capture more long-term connections
		between tokens in a text, not just consecutive chunks.
		<br><br>
		<table class="table table-striped">
			 <thead>
		 		 <tr>
					 	<th>Step</th>
						<th>Result</th>
				</tr>
			 </thead>
			 <body>
			 <tr>
				 <td>Tokenize</td>
				 <td>'for' 'a' 'chance' 'to' 'win' 'a' 'EMOJI' 'NUM' 'cash' 'txt' 'action' 'to' 'NUM' 'u' 'will' 'not' 'be' 'sorry' 'visits' 'URL' 'NUM' 'pm' 'pdt'</td>
			 </tr>
			 <tr>
				 <td>Remove Stop Words</td>
				 <td>'chance' 'win' 'EMOJI' 'NUM' 'cash' 'txt' 'action' 'NUM' 'u' 'not' 'sorry' 'visits' 'URL' 'NUM' 'pm' 'pdt'</td>
				</tr>
				<tr>
 				 <td>Lemmatize</td>
 				 <td>'chance' 'win' 'EMOJI' 'NUM' 'cash' 'txt' 'action' 'NUM' 'u' 'not' 'sorry' 'visit' 'URL' 'NUM' 'pm' 'pdt'</td>
 				</tr>
				<tr>
 				 <td>Add Ngrams</td>
 				 <td>'chance' 'win' 'EMOJI' 'NUM' 'cash' ...'for_a' 'a_chance' 'chance_to' 'to_win' 'win_a' ... 'visits_URL_NUM' 'URL_NUM_pm' 'NUM_pm_pdt'</td>
 				</tr>
		 	</tbody>
	 	</table>
		</div>
		<br>
		<br>
 	 	<b>Counting:</b> finally, we count repeated tokens (ngrams), creating a so-called 'Bag-of-Words' representation.
											Here are top and bottom rows of this BoW expressed as a table:
		<br><br>
		<table class="table table-striped">
			 <thead>
		 		 <tr>
					 	<th>Ngram</th>
						<th>Count</th>
				</tr>
			 </thead>
		<body>
				<tr>
					<td>chance</td>
					<td>1</td>
				</tr>
				<tr>
					<td>to</td>
					<td>1</td>
				</tr>
				<tr>
					<td>win</td>
					<td>1</td>
				</tr>
				<tr>
					<td>EMOJI</td>
					<td>1</td>
				</tr>
				<tr>
					<td>NUM</td>
					<td>3</td>
				</tr>
				<tr>
					<td>cash</td>
					<td>1</td>
				</tr>
				<tr>
					<td>...</td>
					<td>...</td>
				</tr>
				<tr>
					<td>sorry_visits_URL</td>
					<td>1</td>
				</tr>
				<tr>
					<td>visits_URL_NUM</td>
					<td>1</td>
				</tr>
				<tr>
					<td>URL_NUM_pm</td>
					<td>1</td>
				</tr>
				<tr>
					<td>NUM_pm_pdt</td>
					<td>1</td>
				</tr>
			</body>
		</table>
		<br>
		<button onclick="ShowDeets('Deets3')">See Your Bag-of-Words</button>
		<br><br>
		<div id="Deets3">
		<table class="table table-striped">
			 <thead>
		 		 <tr>
					 	<th>Ngram</th>
						<th>Count</th>
				</tr>
			 </thead>
			 <tbody>
					{% for key, value in counter.items() %}
					 	<tr>
						 	<td>{{key}}</td>
							<td>{{value}}</td>
						</tr>
					{% endfor %}
			 </tbody>
		</table>
	</div>
		<br><br>
		<a href="https://github.com/BigBangData/SMS_SpamDetect/blob/74ca298ef7cffccca1350f852766b2401d7a4e91/custom/clean_preprocess.py#L203">Source Code</a>
	</p>
	<p>
		<br>
		<h3>Tranforming the BoW into a Document-Term Frequency Matrix (DTM)</h3>
		<br>
		We need to transform the Bag-of-Words into a numerical vector since ML models mostly take numbers as input.
		There are many ways to represent text as numbers but one of the most basic ways is a
		<a href="https://github.com/BigBangData/NaturalLanguageProcessing/blob/fuji/SMS_SpamDetect/Extra_Document_Term_Matrices.ipynb">Document-Term Frequency Matrix</a>
		where each row is a new document (new text in our case) in a corpus of documents (such as the SMS Collection the model was trained on) and each column is a term
		(a token), and cells contain the term frequency in a document (i.e. the number of times a word is used in a text).
		<br><br>
		<center><img src="../static/img/Explanation2.PNG" alt="Explanation 2" width=50%></center>
		<center><i>Simple DTM on a corpus of three documents</i></center>
		<br><br>
		So far processing a single text in this app is the same as the processing I applied when preparing many texts for the ML model.
		From now on, because we're using a single text with new words and I trained on a fixed corpus of texts and words, the steps differ.
		<br><br>
		For example, the <b>vocabulary of tokens</b> in the SMS Spam Collection is different than the vocabulary of tokens provided to this app. The latter is much smaller,
		yet it might contain new tokens. We need to <b>cast, or project</b> the new tokens into the same shape of the original DTM created during training: <b>same tokens,
		in the same order</b>.If we didn't do this, the ML model would not understand what to do - or rather, it'd just predict nonsense since it doesn't actually
		understand anything.
		<br><br>
		<center><img src="../static/img/Explanation3.PNG" alt="Explanation 3" width=50%></center>
		<center><i>Projecting new document into training DTM</i></center>
		<br><br>
		During training the original vocabulary size had ~40,000 unigram tokens, and would be much larger with bigrams and trigrams. After many tests I selected the most common
		2,000 tokens as the ideal size for my training DTM, balancing a trade-off between accuracy and speed. Below are the first 42 tokens (of 2,000) which were the most common
		in the training data, and their new counts.
		<br><br>
		<table class="table table-striped">
			 <thead>
				 <tr>
					 <th>Training Tokens -> </tg>
					 {% for token in vocabulary %}
						 {% if loop.last %}
						  <th> . . . . </th>
						 {% else %}
						 	<th>{{token}}</th>
						 {% endif %}
			 	 	 {% endfor %}
				 </tr>
			 </thead>
			 <tbody>
					 <tr>
						 <td>New Token Counts -> </td>
						 {% for count in bot %}
							 {% if loop.last %}
							   <td> . . . . </td>
							 {% else %}
						 	   <td>{{count}}</td>
							 {% endif %}
						 {% endfor %}
					 </tr>
			 </tbody>
		</table>
		<br><br>
		<a href="https://github.com/BigBangData/SMS_SpamDetect/blob/74ca298ef7cffccca1350f852766b2401d7a4e91/custom/deploy_models.py#L89">Deployment Code</a>,
		<a href="https://github.com/BigBangData/SMS_SpamDetect/blob/74ca298ef7cffccca1350f852766b2401d7a4e91/custom/clean_preprocess.py#L278">
			WordCounterToVectorTransformer Code</a>
	</p>
	</p>
	<p>
		<br>
		<h3>Converting the DTM into a Document-Term TF-IDF Matrix</h3>
		<br>
		For a single document, the vector of counts consists of mostly 0s since each text has a minuscule fraction of all tokens in all texts.
		This means the information signal in a given vector is very faint. To boost the signal we can leverage more of the information contained
		in the training data. So far we've counted the <b>TF</b> or <b>Term Frequency</b> of a token within a single document, but we can also
		count the <b>DF</b> or <b>Document Frequency</b> of documents within the entire corpus that have that token.
		<br><br>
		We can then pit the TF against the DF by using the inverse of the DF (for details see
		<a href="https://github.com/BigBangData/NaturalLanguageProcessing/blob/fuji/SMS_SpamDetect/Extra_Document_Term_Matrices.ipynb")>this notebook</a>)
		so as to balance out the importance of tokens in a document vs the corpus. Think of it like "removing stop words" from the corpus, if a
		term is very frequent in the corpus, it is not as informative to distinguish documents - TF*IDF boosts the importance of rare terms. (As a side
		note, the fact I'm doing this calculation after selecting the most common 2,000 terms might have defeated its purpose a bit.)
		<br><br>
		A neat property of the TF-IDF calculation is that the resulting values or scores are normalized between 0 and 1. Here's the result given our simple
		corpus. Notice how the TF-IDF scores capture better the meaning of the second document: since "love" and "you" are common in the corpus, their scores
		are a bit lower than the scores for "do" and "not".
		<br><br>
		<center><img src="../static/img/Explanation4.PNG" alt="Explanation 4" width=65%></center>
		<center><i>Conversion from counts to TF IDF scores</i></center>
		<br>
		<button onclick="ShowDeets('Deets4')">See Your DTM</button>
		<br><br>
		<div id="Deets4">
		A non-trivial aspect of this step in the processing pipeline is that it needs to remember the IDF values of the training data and apply a different
		calculation when processing the new text to come up with TF-IDF values. This happens because we cannot calculate IDF values using a corpus of one
		document, since there are no other messages. The table below shows the first 42 counts and TF-IDF scores  in term-document format for ease of comparison).
		<br><br>
		<table class="table table-striped">
			 <thead>
					 <tr>
						 	<th>Token</th>
							<th>Count</th>
							<th>Tfidf</th>
					</tr>
				</thead>
 			 <tbody>
					{% for token, count, tfidf in ziparrays %}
					{% if loop.last %}
					<tr>
						<td>...</td>
						<td>...</td>
						<td>...</td>
					</tr>
					{% else %}
				  		<tr>
						 		<td>{{token}}</td>
								<td>{{count}}</td>
								<td>{{tfidf}}</td>
							</tr>
					 {% endif %}
					 {% endfor %}
			 </tbody>
		</table>
		<br>
		<li>
			<b>Note:</b> If at first you don't see values, try using the tokens in the first column. Repeat the same token and notice what happens to the Tfidf value.
		</li>
	</div>
		<br>
		<a href="https://github.com/BigBangData/SMS_SpamDetect/blob/aae96efb6e5ba9875efa3a2ede8d70958d8e025f/custom/deploy_models.py#L89">Source Code</a>
		<br><br>
	</p>
	<p>
		<br>
		<h3>Reducing Dimensions via Singular Value Decomposition</h3>
		<br>
		While using a TF-IDF representation instead of simple counts boosts the training data's signal, we still have a 2,000-long sparse vector of mostly 0s.
		To solve this problem, a commonly used technique is to project this sparse matrix into what's called a <b>Latent Semantic</b> space using
		a mathematical technique called <b>Singular Value Decomposition  (SVD)</b>. This is a technique used for data compression and other applications. In
		simple terms, SVD reduces a large matrix with lots of columns into a small one with few columns, keeping as much of the original information as possible.
		In our case, we reduce 2,000 tokens into 800 "components" (columns) which capture most of the essence of the tokens into higher-level semantic concepts.
		<br><br>
		This is where explainability starts to fail and the so-called "black box" of machine learning starts. There's no learning yet, we're still just processing
		text, but an SVD projection is a mathematical procedure performed on numeric matrices. The way a human mind groups words into higher-levle concepts such
		as a phrase or a succint explanation of a concept, is conceivably very different than how SVD captures the variance of tokens into a single component
		that might best describe these tokens. In other words: I cannot tell you what the first column of the SVD represents, all I can say is that it is the
		component that explains most of the variance in the data - it captures the most information.
		<br><br>
		The technical details of SVD are beyond the scope of this explanation but for the interested, I recommend
		<a href="https://www.youtube.com/playlist?list=PLMrJAkhIeNNSVjnsviglFoY2nXildDCcv">Prof. Steve Brunton's YouTube Series</a> and his
		<a href="https://www.amazon.com/Data-Driven-Science-Engineering-Learning-Dynamical/dp/1108422098">Data-Driven Science and Engineering book</a>.
		<br><br>
		<button onclick="ShowDeets('Deets5')">See Your SVD</button>
		<br><br>
		<div id="Deets5">
		A <b>low-rank SVD</b> is one which captures the most-informative N components of an SVD projection. After testing low-rank SVD implementations
		<a href="https://github.com/BigBangData/NaturalLanguageProcessing/blob/fuji/SMS_SpamDetect/06_DimensionalityReduction.ipynb">here</a> and
		<a href="https://github.com/BigBangData/NaturalLanguageProcessing/blob/fuji/SMS_SpamDetect/06b_DimensionalityReduction.ipynb">here</a> I ended up
	 	adapting the <i>TruncatedSVD</i> class from Scikit-Learn's ML library
		(<a href="https://github.com/scikit-learn/scikit-learn/blob/95119c13af77c76e150b753485c662b7c52a41a2/sklearn/decomposition/_truncated_svd.py#L25">original</a>,
		<a href="https://github.com/BigBangData/SMS_SpamDetect/blob/aae96efb6e5ba9875efa3a2ede8d70958d8e025f/custom/deploy_models.py#L30">adaptation</a>) compressing
		the 2,000-long matrix into an 800-long one.
		<br><br>
		The reason I adapted the TruncatedSVD class is similar to the reason I kept the IDF values during the TF-IDF calculation. When we apply SVD to the original
		training matrix it is projected into a semantic space wherein SVD components represent higher-level semantic concepts in the training data, not in the new data.
	 	This training SVD space needs to be leveraged during the prediction process - we wouldn't be able to project a single document into this same
		latent semantic space without keeping matrices from the training data to be able to do so (see SVD deployment code below). Here are the first 17 SVD components
		of the 800-long vector, which proportionally correspond to about 42 terms in a 2,000-long vector yet capture a much greater
		 information signal:
		 <br><br>
		 <table class="table table-striped">
 			 <thead>
 					 <tr>
						 <th>Component</th>
 							<th>SVD Projection</th>
 					</tr>
 				</thead>
  			 <tbody>
 					{% for component in svd %}
					{% if loop.last %}
							<tr>
								<td>...</td>
								<td>...</td>
							</tr>
					{% else %}
 				  		<tr>
								<td>{{loop.index}}</td>
 								<td>{{component}}</td>
 							</tr>
					{% endif %}
 					{% endfor %}
 			 </tbody>
 		</table>
	</div>
	<br>
	<a href="https://github.com/BigBangData/NaturalLanguageProcessing/blob/fuji/SMS_SpamDetect/27_PreProcess_TestSet.ipynb">TruncatedSVD adaptation</a>,
	<a href="https://github.com/BigBangData/SMS_SpamDetect/blob/aae96efb6e5ba9875efa3a2ede8d70958d8e025f/custom/deploy_models.py#L108">SVD in deployment</a>
	</p>
	<p>
		<br>
		<h3>Computing Mean Spam Cosine Similarities</h3>
		<br>
		<button onclick="ShowDeets('Deets6')">See Detailed Explanation</button
		<br><br>
		<div id="Deets6">
			<br>
			Cosine similarity is the most common distance metric in natural language processing - it's been proven to be one of the most efficient ways to capture
			the similarity between documents. It is the cosine of the angle between documents represented as vectors in an
			<a href="https://en.wikipedia.org/wiki/Inner_product_space">inner product space.</a> Put simply, if we represent two documents as numeric vectors (as
			we did) we can compute how similar they are using the cosine of the angle between these documents in space. Since visualizing a multidimensional
			space isn't possible, we can build an intuition for this in a 2D Cartesian coordinate system where one axis represents a term and the other another term.
			We can plot documents with two terms as vectors in this plane. Say your corpus was "The Old Man and the Sea" and each document is a sentence in the book,
			yet we're only interested in the words "love" and "sea" since our space is only in 2D. The three vectors below represent three documents containing different
			counts of these words.
			<br><br>
			<center><img src="../static/img/Explanation5.PNG" alt="Explanation 5" width=33%></center>
			<center><i>Documents represented as vectors in a 2D plane</i></center>
			<br>
			The red vector to the left represents a document in which the word "love" happened twice but "sea" only once - this document has more love than sea
			so it angles toward the love axis. The black vector represents a document where "sea" happened six times and "love" twice - it has a lot more sea
			than love so it's closer to the sea axis despite having the same love count. The green vector in the middle is a middle ground between the two terms,
			thus the vector has a 45-degree angle. We can now compute the cosine of the angles between these vectors and compare how similar the documents are
			by how close they are to each other.
			<br><br>
			With other distance metrics (similarity is the opposite of distance, they're two sides of the same coin) such as the ubiquitous Euclidean distance
			("as the crow flies"), the magnitude (length, or word count) of a vector is very important (Euclidean distance squares it). Using Euclidean distance
			for computing document similarity would disproportionately weight how many times sea is mentioned in the black document and put it further away from
			the red document than it need be. By only considering the angle between two vectors the cosine similarity gives preference to the word mix, capturing
			more of the essence of a document.
		</div>
		<br>
		<li><b>How does this help a machine learning model distinguish spam from ham?</b></li>
		<br>
		If we compute the mean cosine similarity between all documents in the training data (which has labels) and build a square
		matrix of cosine similarities between all documents, we can go through each row of this matrix and compute the mean cosine similarity for each
		document (spam or ham) of only the spam columns, creating a "mean spam cosine similarity" feature. The hypothesis is that ham messages will have
		low similarities compared to spam messages, in other words, spam is more similar to spam than ham. This not only stands to reason but it stands
		to visual scrutiny and further validation during model development. Below is the distribution of mean spam cosine similarities for ham and spam
		messages in the training data.
		<br><br>
		<center><img src="../static/img/Explanation6.PNG" alt="Explanation 6" width=50%></center>
		<center><i>Distribution of Ham v. Spam Mean Spam Cosine Similarities</i></center>
		<br>
		<button onclick="ShowDeets('Deets7')">See Your Mean Spam Cosine Similarity</button>
		<br><br>
		<div id="Deets7">
		Notice how small these values are - yet spam is clearly separable from ham. This is only one of the (now 801) features used to predict whether a
		text is ham or spam but it is probably the most powerful predictor. If your value is very close to zero or even negative, it is likely that the model
		will interpret it to be indicative of a legitimate message, but even a value of 0.005 is within the range of possible spam values for this feature:
		</thead>
		<br><br>
 	 	<table class="table table-striped">
 		 	<thead>
 				 <tr>
 					 <th>Mean Spam Cosine Similarity</th>
 				</tr>
 			</thead>
 			<tbody>
 				{% for sim in cossim %}
 						<tr>
 							<td>{{sim}}</td>
 						</tr>
 				{% endfor %}
 		 </tbody>
 		</table>
		</div>
		<br>
		The final processed numeric array is an 801-long vector that consists of the mean spam consine similarity feature stacked onto the
		800-long SVD of the 2,000-long document-term matrix of TF-IDF scores.
		<br><br>
			<a href="https://github.com/BigBangData/NaturalLanguageProcessing/blob/fuji/SMS_SpamDetect/08_CosineSimilarity.ipynb">Cosine Similarity Notebook</a>,
			<a href="https://github.com/BigBangData/SMS_SpamDetect/blob/e0afac56b5b0569c431b2607ef24b83f7ed67390/custom/deploy_models.py#L113">Deployment Code</a>
		<br>
	</p>
	<p>
	<br>
	<h2 style="color:#3c90b5; text-align:center">Feeding The Processed Text Into A Model</h2>
	<br>
	All processing steps and feeding the transformed text into a pre-trained model are encapsulated in the first line of code below,
	and all the ML model development and training and the final prediction are encapsulated in the second line of code. These two
	little lines accomplish a lot under the hood. We've covered the first line, now we'll cover the second.
	<br><br>
	<center><img src="../static/img/Explanation7.PNG" alt="Explanation 7" width=33%></center>
	<center><i>Preprocessing and predicting in
		<a href="https://github.com/BigBangData/SMS_SpamDetect/blob/25203fc4aa0b6d2497dcd3d6993f706d1b9d1438/spam-detect42.py#L37">
			two lines of code</a></i></center>
	<br><br>
	This XGBoost model above is a pre-trained model selected from dozens of models trained over months and evaluated thoroughly for
	their ability to generalize beyond the training data. The pre-trained model was saved in and is loaded from
	<a href="https://github.com/BigBangData/SMS_SpamDetect/tree/main/data/5_deployment">this repository</a>.
	<br><br>
	Since there really isn't much more to feeding a processed text into a model, in each section below I offer some tidbits about
	the NLP phase I found interesting and that interfered with my assumptions of ML projects and with my solo project management
	skills (or lack thereof).
  <br><br>
	<button onclick="ShowDeets('Deets8')">Memory Details</button>
	<br><br>
	<div id="Deets8">
		The model has a surprisingly light memory footprint at 106KB compared to two of the processing files,
		especially the SVD transformer which gets used in the new SVD projection calculation and which takes up 47.3MB of space in my local machine.
		<center><img src="../static/img/Explanation8.PNG" alt="Explanation 8" width=33%></center>
		<center><i>Saved preprocessing and modeling file sizes</i></center>
		<br>
		I found this curious because I thought of machine learning as the model-building and model-predicting phases of a project,
		yet in NLP there's some learning happening during the text processing phase - as we can see by all the bits and pieces saved up
		because of the information accumulated there.
		<br><br>
	</div>
	<button onclick="ShowDeets('Deets9')">Some ML Details</button>
	<br><br>
	<div id="Deets9">
		The most common definition of ML is that of <a href="http://www.cs.cmu.edu/~tom/">Prof. Tom Mitchell</a>:
			<blockquote>
			<i>A machine is said to learn from experience E with respect to some class of tasks T and performance measure P, <br>
			if its performance at tasks in T, as measured by P, improves with experience E.</i>
			</blockquote>
		While I evaluated the accuracy of numerous processing steps in small iterative model evaluation phases using simple models - this
		is hardly machine learning, more like human learning. It is also fallible, carrying an assumption that a simple model's evaluation
		transfers to a complex model. I noticed this fallibility when I evaluated whether to use TF-IDF or SVD steps or not with simple vs
		complex models and saw that more complex structures performed better with more complex models, and vice-versa. Accounting for
		<a href="https://en.wikipedia.org/wiki/Overfitting">overfitting and generalization</a>, I was able to confirm that, at least with this
		data and in this project, I could achieve better results with more complex structures and models.
		<br><br>
		Below is an example of a quick evaluation phase for the vocabulary size and number of ngrams as far as accuracy
		 (which is not the best metric as it is too vague), using a simple Logistic Classifier - the peak accuracy plateau around 2,000
		 trigrams explains why I chose these specific values, yet it is very well possible other values would work better with XGboost:
		<br><br>
		<center><img src="../static/img/Explanation9.PNG" alt="Explanation 9" width=60%></center>
		<center><i>Quick evaluation phase during processing</i></center>
		<br><br>
	</div>
	<button onclick="ShowDeets('Deets10')">Project Management</button>
	<br><br>
	<div id="Deets10">
		Just in the processing phase, the combination of possible vocabulary sizes, ngram variations, representation types (TF-IDF vs Counts, SVD or not,
		adding text lengths or cosine similarities or not) quickly exploded to nearly a thousand - and this before any modeling work.
		Without project management guildelines, common sense, and a bit of art, finding the best solution given a dataset can be daunting.
		The project was developed in
		<a href="https://github.com/BigBangData/NaturalLanguageProcessing/tree/fuji/SMS_SpamDetect">this GitHub repository</a> and I managed the project
		more specifically in
		<a href="https://github.com/BigBangData/NaturalLanguageProcessing/blob/fuji/SMS_SpamDetect/01_Project_Management.ipynb">this Notebook.</a>
		<br><br>
		Below is another quick processing evaluation phase that suffers from the same possible issue of generalization to complex models.
		The representation I settled on after more tests was the <b>X_bot_tfidf_svd_cos</b> (Bag-of-upto-Trigrams, TF-IDF, SVD + Cosine similarity
		feature).
		<br><br>
		<center><img src="../static/img/Explanation10.PNG" alt="Explanation 10" width=50%></center>
		<center><i>Quick Evaluation of 12 Representations with a Baseline Logistic Classifier</i></center>
		<br><br>
	</div>
	</p>
	<p>
	<br>
																		<h2 style="color:#3c90b5; text-align:center">Making A Prediction</h2>
	<br>
	The way a model predicts is related to how it learns and what kind of model it is. In this section I'll go into some of the concepts
	of statistical learning, which models I trained, and why I chose XGBoost for this app.
	<br><br>
	<h2 style="color:#9c275d;">Statistical Learning</h2>
	<h3>What is a model?</h3>
	There are many kinds of models: mental, mathematical, physical, statistical, predictive, inferential, causal, and so forth.
	A model is a simplified version of reality. For the most part, we simplify reality to make sense of it and predict what is likely to happen.
	Too simple a model and its understanding and predictions will generally be <b>biased</b>; too complex, and they'll generally suffer from <b>variance</b>.
	 This is called the
	<a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">bias-variance tradeoff</a> and is one of the central problems in data science.
	Getting the model "just right" means to find the balance these two opposing forces such that the model is able to generalize
	beyond the data it sees.
	<br><br>
	<h3>Why do models need data?</h3>
	A model relies on some kind of experience or knowledge of reality, which can be measured and captured as data. Reality is vast and diverse,
	and data captures only a fraction of it, so the data capture process has to be carefully conceived and conducted - if your data is garbage,
	no amount of modeling work will turn it into diamonds.
	<br><br>
	<button onclick="ShowDeets('Deets11')">Statistical Modeling</button>
	<br><br>
	<div id="Deets11">
		In statistical modeling, we aim to "fit" the data somehow. When models <b>underfit</b> the data they're biased, when they
		<b>overfit</b> they have too much variance. In both cases they don't generalize - with bias it is because the models fail to capture the
		variance in the data well, they fail to learn from the data all they can; with variance it is because the models capture too much of the
		variance in the data, they learn too well what they've been shown so that when they're shown new things they don't have the flexibility
		to adapt and compare to what they've seen.
		<br><br>
		Say you grew up in a community that only had wooden chairs and your model of a chair is that it needs to be made of wood. In this case, your
		model overfits the data it's seen, specifically, for an instance of a chair, the feature "material" is not helping generalize. When you move
		and see plastic chairs you will be unable to understand or predict these objects to be chairs. On the other hand, had your model been so
		simple and general that a chair meant "any object you can sit on", your model would be biased and when given a couch or stool, it'd have
		predicted those to be chairs as well. In this case features such as "size" or "has a back" were missing from the model. Training a model
		includes showing it instances (examples) of objects which might be chairs or not, and showing various attributes or features of those objects,
		and getting the model to estimate whether they are chairs or not. The more instances and the more features, the more data in other words, the
		better the chances our models will be able to understand reality and predict well.
		<br><br>
	</div>
	<h3>What are the main types of statistical models?</h3>
	<button onclick="ShowDeets('Deets12')">See Details </button>
	<br><br>
	<div id="Deets12">
	  Models learn in different ways, for example, there are <b>unsupervised</b> and <b>supervised</b> models. The main difference is that supervised
		models have a "label" or "target" attribute they're aiming to predict, such as the chair example above. Unsupervised ones have no label - for
		example, we could have a ton of objects of various types without any labels and we could just throw all of these objects into a model that
		groups these objects into "natural groups given the data" - meaning, we don't teach it what groups, we don't label those groups. This is helpful
		when we either don't know the groups ahead of time or want to learn about the data, such as in an exploratory data analysis phase of a project.
		<br><br>
		This project uses <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised learning</a> to predict spam. There are <b>regression</b>
		and <b><a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a></b> tasks that supervised models perform - the former
		assumes that the label or target of prediction is continuous (such as predicting mpg from a vehicle's weight and number of cylinders) and the latter,
		which is the case of the chair and our spam case, assumes that the target has discrete levels that can be easily counted (the distinction is more
		practical than theoretical). The simplest classificaion case is binary: 1 or 0, yes or no, spam or ham.
		<br><br>
		Another characteristic of this project is that it used <b>classical ML algorithms</b> and not neural networks or deep learning. I used the Python
		 <a href="https://scikit-learn.org/stable/modules/classes.html">Scikit-Learn API</a> and my own custom adaptations from it and from other sources
		 (see the acknowledgements at the end). I avoided neural nets and DL mostly because the data is small and I knew classical algorithms would
		 arrive at optimal solutions given the data, in other words, the main barrier to generalization would be the data not the model.
		<br><br>
	</div>
	<h3>How do supervised models learn?</h3>
	<button onclick="ShowDeets('Deets13')">See Learning Details</button>
	<br><br>
	<div id="Deets13">
		A supervised model learns with examples (rows, or instances) that have attributes (columns, or features) and labels (a target). Let's consider
		a simple example of binary classification using the target "is poisonous" (1=yes, 0=no) in the famous
		<a href="https://archive.ics.uci.edu/ml/datasets/mushroom">mushroom dataset</a>. Notice how the features are easy to interpret, here are a few
		instances and select features:
		<br><br>
		<center><img src="../static/img/Explanation11.PNG" alt="Explanation 11" width=30%></center>
		<center><i>Learning labels (poisonous or not) from instances (mushrooms) and features (shape, color, odor, ...)</i></center>
		<br><br>
		For every mushroom a model uses the features to predict whether it is poisonous or not. This is easily interpretable compared to the spam detection
		case. With text data, the tranformations make it harder to interpret but the principle is the same - each feature is an attribute or characteristic
		of the instance (a text) that captures some information on whether that instance is spam or not. It just so happens that our features are SVD
		components, 800 of them (plus the cosine similarities):
		<br><br>
		<center><img src="../static/img/Explanation12.PNG" alt="Explanation 12" width=60%></center>
		<center><i>Learning labels (spam or not) from instances (documents) and features (SVD values)</i></center>
		<br><br>
		The label is hidden and the first predictions are blind guesses. When training the model we show the correct label and tell the model how wrong it was.
		This isn't as black and white as "dear model, you gave me a 1, but it is a 0, you're 100% wrong: <i>bad model</i>", since most models make probability
		estimates as to whether the label is 1. Say the model predicts 0.9 and the label is 1, then it's only wrong by 0.1, but if it predicts 0.5, the
		error is four times larger. The model is then penalized according to the amount of error it makes.
		<br><br>
		Penalizing a model happens at the feature level so that the model learns which features are more (or less) correct and adjusts "weights"
		 given to those features. A weight is a number that multiplies a given feature, giving it more (or less) importance.
		 Generally only few features turn out to be great predictors and most features end up being discarded entirely or just reduced to having very
		 little influence over the output (the prediction). In the mushroom dataset, odor is one of the best predictors, but not so much shape and color.
		<br><br>
	</div>
	<h3>How do models learn to predict beyond the data they see?</h3>
	<button onclick="ShowDeets('Deets14')">See Learning Details</button>
	<br><br>
	<div id="Deets14">
		<li><b>Train, Text Split</b></li>
		Predicting the future is very hard. We mimic the process by splitting our data into chunks and pretending an unseen chunk is "the future".
		At the beginning of the project we make a basic split into <b>training data</b> (usually approx. 80%) and <b>test data</b> - and we do not peek much
		into the test data, since that would mean peeking into the future. We save it for the final model evaluation, to see how it'd likely
		perform with completely unseen data. We do this only once.
		<br><br>
		<li><b>Cross Validation</b></li>
		With the rest of the training data, we split it into various chunks, called "folds", generally 5 or 10 folds. We take care that each fold is representative
		of the data: for example, stratification is used to preserve the "class imbalance" (which in our case was 13/87 meaning, 13% spam and 87% ham) and randomized
		methods are used to select random instances from the data. We hide one of the folds from the model - again, we pretend that fold is the future. In 10-fold
		<a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)">cross validation</a> we train the model on 9 folds and test its predictions on the 10th
		or held-out fold. We do this 10 times iterating through all the folds and take measures of central tendency (mean) and variance (standard deviation) to be able
		to determine with more robustness how well the model generalizes. Certain models might achieve high accuracy but might also have too much variance, they
		might also on occasion have quite low accuracy, so we prefer models with low variance.
		<br><br>
	</div>
	<h2 style="color:#9c275d;">Models Trained</h2>
	 I trained a variety of models including Support Vector Machines, Decisions Trees, Random Forests, AdaBoost and Gradient Boosting Classifiers,
	 Voting Classifiers, and an XGBoost model at the very end. Most finalists did quite well during both cross-validation and model evaluation,
	 achieving 99% accuracy and high scores in other classification metrics as well, such as sensitivity and specificity. In the following notebooks
	 I performed model evaluation using
	 <a href="https://github.com/BigBangData/NaturalLanguageProcessing/blob/fuji/SMS_SpamDetect/28_EvaluateModels_1.ipynb">single confusion matrices</a>,
	 <a href="https://github.com/BigBangData/NaturalLanguageProcessing/blob/fuji/SMS_SpamDetect/29_EvaluateModels_2.ipynb">learning curves</a>, by looking at
	 <a href="https://github.com/BigBangData/NaturalLanguageProcessing/blob/fuji/SMS_SpamDetect/30_EvaluateModels_3.ipynb">prediction examples</a>,
	 and more fully with
	 <a href="https://github.com/BigBangData/NaturalLanguageProcessing/blob/fuji/SMS_SpamDetect/31_EvaluateModels_4.ipynb">classification reports.</a>
	<br><br>
	<li><b>Why did this app fail to predict my text?</b></li>
	<br>
	With this app I pursued learning how various models work and explaining the final end-to-end process, not searching for great labeled data, which is hard to
	come by and potentially expensive. A production implementation that provides business value would be entirely different, deal with scaling issues, massive
	datasets, and potentially different algorithms although XGBoost scales well. The main pitfall of this app is the small dataset. I trained the models on 3,900
	instances, which is clearly not enough to be future proof. If a prediction failed it is likely because it was vastly different than anything in the SMS Spam
	Collection.
	<br><br>
	In my brief experience using the app, I suspect it overfits on a couple of main features. I'd imagine there are a couple of features (SVD components) that represent
	the "numberiness" and "lengthiness" of a text. So the more numbers you use or the longer your text is, the likelier it is be predicted as spam. Despite this you
	might find that short texts with no numbers can be classified as spam, if they include spam-like bigrams - try "call now for a free quote"?
	<br><br>
	<h2 style="color:#9c275d;">XGBoost</h2>
	I chose XGBoost for the app, despite being a latecomer in the modeling effort, because it performed really well out of the box, having one of the lowest variances
	and yet comparably high scores, and a relatively quick prediction time as well. To be fair, it takes longer to predict, but it still is quite fast and well under
	a second so this wasn't an issue. See details in
	<a href="https://github.com/BigBangData/NaturalLanguageProcessing/blob/fuji/SMS_SpamDetect/31_EvaluateModels_4.ipynb">this notebook</a>.
	<br>
	<h3>How does XGBoost work?</h3>
	XGBoost stands for <b>Extreme Gradient Boosting</b> which sounds extreme but in fact it is a more
	<a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">regularized</a> version of <b>Gradient Boosting.</b> Regularization is the process of
	balancing the bias-variance tradeoff. (Boosting models tend to learn very well and overfit quite badly so figuring out a good regularization method is imperative.)
	XGBoost is not exactly an algorithm but rather an "open-source software library which provides a gradient boosting framework" (
	<a href="https://en.wikipedia.org/wiki/XGBoost">Wikipedia</a>, accessed 3/27/2021) that has become popular as the library of choice for many winning ML teams in
	recent competitions.
	<br><br>
	To delve into XGBoost we need to learn several topics, chief among them, in reverse order:
	<ul>
		<li><a href="https://en.wikipedia.org/wiki/Gradient_boosting">Gradient Boosting</a></li>
		<li><a href="https://en.wikipedia.org/wiki/Gradient">Gradient</a></li>
		<li><a href="https://en.wikipedia.org/wiki/Boosting_(machine_learning)">Boosting</a></li>
		<li><a href="https://en.wikipedia.org/wiki/Random_forest">Random Forests</a></li>
		<li><a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating">Bagging</a></li>
		<li><a href="https://en.wikipedia.org/wiki/Ensemble_learning">Ensemble Learning</a></li>
		<li><a href="https://en.wikipedia.org/wiki/Decision_tree_learning">Decision Trees</a></li>
	</ul>
	<br>
	<button onclick="ShowDeets('Deets15')">Decision Trees</button>
	<br><br>
	<div id="Deets15">
		A decision tree is a highly interpretable type of model - yet its main flaw is not being robust: given slightly different initial
		conditions (decision trees take random seeds as an initialization parameter) they might turn out to be completely different. Yet because
		the decisions a tree makes are so interpretable, despite the lack of rubustiness, these models are often used in the industry when explanatory
		power is needed. For a deep dive into this type of model, I cannot recommend enough Josh Starmer's
		<a href="https://www.youtube.com/playlist?list=PLblh5JKOoLUKAtDViTvRGFpphEc24M-QH">CART (Classification And Regression Trees) YouTube playlist</a>,
		which is unbeatable at explaining Decision Trees (Josh's StatQuest Channel is unbeatable, period). Here I'll just cover the bare bones.
		<br><br>
		A decision tree is so intuitive it is easier to start by looking at the results of a classification tree than to explain how it arrives at those results.
		Using this quickly made up dataset I constructed a simplistic decision tree to classify whether a patient is fit or unfit for a particular type of
		exam:
		<br><br>
		<center><img src="../static/img/Explanation13.PNG" alt="Explanation 13" width=25%></center>
		<center><i>Toy dataset for classifying patients based on a few features.</i></center>
		<br><br>
		A tree's classifications can be explained by a series of decisions starting from a root note and splitting on yes/no branches into lower nodes and leaves.
		Nodes present further splits while leaves are final classifications. The tree algorithm arrives at optimal splitting criteria such as having 2.5 veggie
		servings or less below. Skipping the details of how algorithms arrive at notes and splitting criteria, the important thing to notice with single decision
		trees	is how they vary significantly in shape depending on a random initialization of the algorithm (see more details below). I've used two slightly
		different seeds and produced very different trees.
		<br><br>
		<center><img src="../static/img/Explanation14.PNG" alt="Explanation 14" width=65%></center>
		<center><i>Graphs: Decision Structure & Classification Boundaries on a Plane.</i></center>
		<br><br>
		Tree algorithms seek to reduce the <b>impurity of a node</b> until they can't anymore or some hyperparameter (a parameter specified by the user to
		regulate the tree's behavior) tells it to stop before overfitting. Impurity is a measure of the "mix of classes" in a given node, or diversity of a node:
		the least diverse, the more pure. There are many ways to measure impurity but the most common are <b>gini</b> and <b>entropy</b>. Entropy is a measure of
		disorder of a system so the higher the entropy the more impure or mixed. We see that in our example, since I had a 50/50 mix, the entropy of the root node
		is 1 (the highest), and since the trees were able to perfectly classify the instances, the leaves are pure (entropy=0).
		<br><br>
		We can see thus how entropy decreases at each node. If a split increases entropy, it is not made and that node becomes a leaf. With more realistic and complex
		data, exhaustively searching features on which to split, and exaustively searching splitting criteria becomes unfeasible, so heuristic algorithms are the
		norm, using random searches. Algorithms are initialized with a pseudorandom seed (as is common in many ML algorithms) and single trees are not perfect
		solutions but good enough solutions. Leaves are often partly impure and that is okay, in fact we prune trees so that they generalize better.
		<br><br>
		We can also see how trees divide the data into rectangles. Notice how the first split is horizontal and the next split is vertical in the plots above.
		If your data looked like a parabola, a tree would be a bad choice of model, although possible. Trees are great at nonlinear classification in higher dimensions so
		they are great for many real-world problems - the only problem with single trees is how they are innacurate and cannot predict the future well, but what
		we lose in prediction is gained in interpretability, a common tradeoff in ML.
		<br><br>
	</div>
	<br>
	<button onclick="ShowDeets('Deets16')">Ensemble Learning</button>
	<br><br>
	<div id="Deets16">
		A solution to the problem of inflexible and inaccurate single trees is to combine many trees into a forest, get all their predictions and take a vote.
		This draws on the principle of the "wisdom of the crowds" - briefly, that if you get an answer from many and independent people about a topic, they'll
		make different kinds of errors (because they have different biases) arriving at the answer, but the central tendency (could be the mean, or mode) will
		be a lot more accurate than an individual answer. This is easily demonstrated in street fairs with jelly bean jars by asking random people to guess
		the number of beans. The errors cancel themselves out and the crowd often guesses with high accuracy. The general class of methods for this is called
		<a href="https://en.wikipedia.org/wiki/Ensemble_learning">ensemble learning</a> and below we will look at some of the particular methods in this class.
		<br><br>
	</div>
	<br>
	<button onclick="ShowDeets('Deets17')">Bagging</button>
	<br><br>
	<div id="Deets17">
		One form of ensemble learning is called Bagging, which is short for
		<a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating"><b>B</b>ootstrap <b>Agg</b>regat<b>ing</b></a>.
		<a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">Bootstrapping</a> is basically "random sampling with replacement", a very powerful
		technique to generate different samples from the same data. The important part is the replacement part: you can end up with repeated instances. Say
		you are bootstrapping from six dice throws, you might end up with a dataset of: 2, 3, 1, 1, 3, 6. If you were not replacing you would by definition
		end up with all six, say: 2, 3, 1, 4, 6, 5. If you create various datasets that aren't bootstrapped they will all contain the same samples, so in
		essence (unless order matters for some reason) they are all the same dataset. If you bootstrap, they might all be slightly different datasets. This means
		we can, with the same data, bootstrap ourselves (pick ourselves up by our bootstraps, like Baron Munchausen) into creating many different datasets.
		Sounds like magic, but it works.
		<br><br>
		Aggregating the results of many bootstrapped samples of our data and taking a majority vote or some other measure of central tendency we can therefore
		reduce variance and improve the stability of predictions. Bagging can be used with many procedures but it is commonly applied to decision trees because
		of their inherent instability.
	</div>
	<br>
	<button onclick="ShowDeets('Deets18')">Random Forests</button>
	<br><br>
	<div id="Deets18">
		A <a href="https://en.wikipedia.org/wiki/Random_forest">Random Forest</a> uses a type of bagging applied to decision trees that takes a subsample of features
		from the data so as to increase the independence of the trees. This is especially important when there are many features (such as in most NLP cases) and
		when there's likelihood that many of these featuers are correlated (see <a href="https://en.wikipedia.org/wiki/Multicollinearity">multicollinearity</a>).
		The number of features to use is a hyperparameter that is controlled by the user (say, a data scientist training a model) so as to find the optimal
		number for a given problem.
		<br><br>
		Josh Starmer's
		<a href="https://www.youtube.com/playlist?list=PLblh5JKOoLUIE96dI3U7oxHaCAbZgfhHk">Random Forests YouTube playlist</a> is again a great source for learning
		more about the details such as how at each node all features are considered, and how not all instances are actually used in bagging so we end up with an
		"out-of-bag" dataset of unused instances that can serve as a form of validation. (One caveat is that if you tune your hyperparameter using the OOB dataset
		you will in essence overfit this dataset so you still need a solid CV scheme to avoid overfitting). I spend a great deal of time using random forests during
		training and the finalists arrived at very similar metrics during evaluation to boosting methods, which are generally found to be more accurate.
	</div>
	<br>
	<button onclick="ShowDeets('Deets19')">Boosting</button>
	<br><br>
	<div id="Deets19">
	</div>
	<br>
	<button onclick="ShowDeets('Deets20')">Gradient and Gradient Boosting</button>
	<br><br>
	<div id="Deets20">
	</div>
	XGBoost history and links...
	<a href="https://arxiv.org/abs/1603.02754">2016 paper</a>
	<a href="https://github.com/dmlc/xgboost">GitHub repo</a>
	</p>
	<p>
	<br>
	<h2 style="color:#3c90b5; text-align:center">Rendering The Prediction</h2>
	<br><br>
	IN PROGERSS...
	<br><br>
	</p>
	<p>
		<h3>Acknowledgements</h3>
		<br>
		I'm indebted to various books, tutorials, and blogs online that I consulted along the way, see a full list of
		<a href="https://github.com/BigBangData/SMS_SpamDetect#acknowledgements">acknowledgements here.</a>
		To get this app off the ground, I'm indebted to Chayan Kathuria's tutorial on how to build and deploy a spam classifier app on heroku
		<a href="https://towardsdatascience.com/build-deploy-a-spam-classifier-app-on-heroku-cloud-in-10-minutes-f9347b27ff72">in 10 minutes</a>
		(it took me a couple evenings). For getting me unstuck and suffering through my atrocious css and js scripts I'm indebted to
		Information Systems Management Researcher <a href="https://daveeargle.com/">Dave Eargle</a>.
		<br><br>
	</p>
	<script>
	function ShowDeets(Id) {
	  var x = document.getElementById(Id);
	  if (x.style.display === "block") {
	    x.style.display = "none";
	  } else {
	    x.style.display = "block";
	  }
	}
	</script>
</body>
</html>
<footer>
	<div>
		<h2 style="color:#3c90b5; text-align:center"><a style="text-decoration:none" href="/">Try Predicting Spam Again!</a></h2>
	<br><br><br>
	</div>
</footer>
