<!DOCTYPE html>
<html>
<head>
	<title>Results</title>
    <link rel="stylesheet" type="text/css" href="../static/css/styles.css">
</head>
<body>
	<header>
		<div class="container">
			<h2>Spam Detector For SMS Messages</h2>
		  <h3>A Machine Learning App That Explains It All</h3>
		</div>
	</header>
	<div class="results">
		{% if prediction == 1%}
			<h2 style="color:#e0392d;">That was not a nice message...</h2>
		{% elif prediction == 0%}
			<h2 style="color:#3d7aba;">Thank you, that <i>was</i> a nice message.</h2>
		{% endif %}
	</div>
	<p>
		<h3>How did the machine learning algorithm arrive at this prediction from your text message?</h3>
		<br>
		At a high level, these are the main steps in this app:
		<br><br>
		<center><img src="../static/img/Explanation1.PNG" alt="Explanation 1" width=95%></center>
		<p style="font-size:10px" align="center">
			Interior of a classroom, students bent at study, Yallourn, 1947.
			<br>
 			Photo by <a href="https://unsplash.com/@museumsvictoria">Museums Victoria</a> in <a href="https://unsplash.com/">Unsplash</a>.
		</p>
		<br>
		<h2 style="color:#3c90b5;">Processing The Text</h2>
		Most of the time is spent processing the text as this numeric array that the ML model can understand. Prediction happens quickly after that. Processing takes several steps:
		<ul>
			<li>Creating Bag of Words (BoW)</li>
			<li>Tranforming the BoW into a Document-Term Frequency Matrix (DTM)</li>
			<li>Changing the DTM into a Document-Term TF-IDF Matrix</li>
			<li>Reducing Dimensions via Singular Value Decomposition</li>
			<li>Computing Mean Spam Cosine Similarities</li>
		</ul>
		<br>
		As a motivating example, let's follow this spam text taken from our collection (with some modifications):
		<p style="font-family:monospace; text-align:center">
			For a chance to win a å£250 cash TXT: ACTION to 80608. U won't be sorry -visits @www.movietrivia.tv @8pm PDT!
		</p>
		<br>
		<h3>Creating a Bag of Words (BoW)</h3>
		<br>
		<b>Parsing:</b> we cleanup the text, that is: lower case, remove  punctuation, replace URLs and numbers, and so forth.
		<br><br>
		<button onclick="ShowDeets1()">See Details</button>
		<br>
		<div id="Deets1">
		<br>
		These are some of the steps performed, for a deeper look into all the preprocessing see the source code (link below).
		<br><br>
			<table class="table table-striped">
				 <thead>
			 		 <tr>
						 	<th>Step</th>
							<th>Result</th>
					</tr>
				 </thead>
				 <tbody>
						 	<tr>
								<td>Expand Contractions</td>
								<td>For a chance to win a å£250 cash TXT: ACTION to 80608. U will be sorry -visits @www.movietrivia.tv @8pm PDT!</td>
							</tr>
							<tr>
								<td>Lower Case</td>
								<td>for a chance to win a å£250 cash txt: action to 80608. u will not be sorry -visits @www.movietrivia.tv @8pm pdt!</td>
							</tr>
							<tr>
								<td>Replace URLs</td>
								<td>for a chance to win a å£250 cash txt: action to 80608. u will not be sorry -visits  URL  @8pm pdt!</td>
							</tr>
							<tr>
								<td>Replace Numbers</td>
								<td>for a chance to win a å£ NUM  cash txt: action to  NUM . u will not be sorry -visits  URL  @ NUM pm pdt!</td>
							</tr>
							<tr>
								<td>Remove Punctuation</td>
								<td>for a chance to win a å NUM cash txt action to NUM u will not be sorry visits URL NUM pm pdt</td>
							</tr>
							<tr>
								<td>Replace Emojis</td>
								<td>for a chance to win a  EMOJI  NUM cash txt action to NUM u will not be sorry visits URL NUM pm pdt</td>
							</tr>
				 </tbody>
			</table>
		</div>
		<br>
		<b>Tokenizing:</b> we split the text into chunks (tokens), remove common words, lemmatize, and group into ngrams.
		<br>
		<br>
		<button onclick="ShowDeets2()">See Details</button>
		<div id="Deets2">
		<br>
		Very common words ("stop words") are removed as they provide little signal, and words are
		<a href="https://en.wikipedia.org/wiki/Lemmatisation">lemmatized</a> so as to standardize some of the variety and try to capture more signal.
		We then form two and three-long sequences of tokens (bigrams and trigrams) to capture some of the order in the text.
		These steps attempt to capture some of the meaning behind a message. There are many other ways to do this, for example, with deep learning architectures
		and <a href="https://en.wikipedia.org/wiki/Long_short-term_memory">LSTM (Long-Short-Term-Memory)</a> cells I could've capture more long-term connections
		between tokens in a text, not just consecutive chunks.
		<br><br>
		<table class="table table-striped">
			 <thead>
		 		 <tr>
					 	<th>Step</th>
						<th>Result</th>
				</tr>
			 </thead>
			 <body>
			 <tr>
				 <td>Tokenize</td>
				 <td>'for' 'a' 'chance' 'to' 'win' 'a' 'EMOJI' 'NUM' 'cash' 'txt' 'action' 'to' 'NUM' 'u' 'will' 'not' 'be' 'sorry' 'visits' 'URL' 'NUM' 'pm' 'pdt'</td>
			 </tr>
			 <tr>
				 <td>Remove Stop Words</td>
				 <td>'chance' 'win' 'EMOJI' 'NUM' 'cash' 'txt' 'action' 'NUM' 'u' 'not' 'sorry' 'visits' 'URL' 'NUM' 'pm' 'pdt'</td>
				</tr>
				<tr>
 				 <td>Lemmatize</td>
 				 <td>'chance' 'win' 'EMOJI' 'NUM' 'cash' 'txt' 'action' 'NUM' 'u' 'not' 'sorry' 'visit' 'URL' 'NUM' 'pm' 'pdt'</td>
 				</tr>
				<tr>
 				 <td>Add Ngrams</td>
 				 <td>'chance' 'win' 'EMOJI' 'NUM' 'cash' ...'for_a' 'a_chance' 'chance_to' 'to_win' 'win_a' ... 'visits_URL_NUM' 'URL_NUM_pm' 'NUM_pm_pdt'</td>
 				</tr>
		 	</tbody>
	 	</table>
		</div>
		<br>
		<br>
 	 	<b>Counting:</b> finally, we count repeated tokens (ngrams), creating a so-called 'Bag-of-Words' representation. Here are top and bottom rows of this BoW expressed as a table:
		<br><br>
		<table class="table table-striped">
			 <thead>
		 		 <tr>
					 	<th>Ngram</th>
						<th>Count</th>
				</tr>
			 </thead>
		<body>
				<tr>
					<td>chance</td>
					<td>1</td>
				</tr>
				<tr>
					<td>to</td>
					<td>1</td>
				</tr>
				<tr>
					<td>win</td>
					<td>1</td>
				</tr>
				<tr>
					<td>EMOJI</td>
					<td>1</td>
				</tr>
				<tr>
					<td>NUM</td>
					<td>3</td>
				</tr>
				<tr>
					<td>cash</td>
					<td>1</td>
				</tr>
				<tr>
					<td>...</td>
					<td>...</td>
				</tr>
				<tr>
					<td>sorry_visits_URL</td>
					<td>1</td>
				</tr>
				<tr>
					<td>visits_URL_NUM</td>
					<td>1</td>
				</tr>
				<tr>
					<td>URL_NUM_pm</td>
					<td>1</td>
				</tr>
				<tr>
					<td>NUM_pm_pdt</td>
					<td>1</td>
				</tr>
			</body>
		</table>
		<br>
		<button onclick="ShowDeets3()">See Your Bag-of-Words</button>
		<br><br>
		<div id="Deets3">
		<table class="table table-striped">
			 <thead>
		 		 <tr>
					 	<th>Ngram</th>
						<th>Count</th>
				</tr>
			 </thead>
			 <tbody>
					{% for key, value in counter.items() %}
					 	<tr>
						 	<td>{{key}}</td>
							<td>{{value}}</td>
						</tr>
					{% endfor %}
			 </tbody>
		</table>
	</div>
		<br><br>
		<a href="https://github.com/BigBangData/SMS_SpamDetect/blob/74ca298ef7cffccca1350f852766b2401d7a4e91/custom/clean_preprocess.py#L203">Source Code</a>
	</p>
	<p>
		<br>
		<h3>Tranforming the BoW into a Document-Term Frequency Matrix (DTM)</h3>
		<br>
		We need to transform the Bag-of-Words into a numerical vector since ML models mostly take numbers as input.
		There are many ways to represent text as numbers but one of the most basic ways is a
		<a href="https://github.com/BigBangData/NaturalLanguageProcessing/blob/fuji/SMS_SpamDetect/Extra_Document_Term_Matrices.ipynb">Document-Term Frequency Matrix</a>
		where each row is a new document (new text in our case) in a corpus of documents (such as the SMS Collection the model was trained on) and each column is a term
		(a token), and cells contain the term frequency in a document (i.e. the number of times a word is used in a text).
		<br><br>
		<center><img src="../static/img/Explanation2.PNG" alt="Explanation 2" width=50%></center>
		<center><i>Simple DTM on a corpus of three documents</i></center>
		<br><br>
		So far processing a single text in this app is the same as the processing I applied when preparing many texts for the ML model.
		From now on, because we're using a single text with new words and I trained on a fixed corpus of texts and words, the steps differ.
		<br><br>
		For example, the <b>vocabulary of tokens</b> in the SMS Spam Collection is different than the vocabulary of tokens provided to this app. The latter is much smaller,
		yet it might contain new tokens. We need to <b>cast, or project</b> the new tokens into the same shape of the original DTM created during training: <b>same tokens,
		in the same order</b>.If we didn't do this, the ML model would not understand what to do - or rather, it'd just predict nonsense since it doesn't actually
		understand anything.
		<br><br>
		<center><img src="../static/img/Explanation3.PNG" alt="Explanation 3" width=50%></center>
		<center><i>Projecting new document into training DTM</i></center>
		<br><br>
		During training the original vocabulary size had ~40,000 unigram tokens, and would be much larger with bigrams and trigrams. After many tests I selected the most common
		2,000 tokens as the ideal size for my training DTM, balancing a trade-off between accuracy and speed. Below are the first 42 tokens (of 2,000) which were the most common
		in the training data, and their new counts.
		<br><br>
		<table class="table table-striped">
			 <thead>
				 <tr>
					 <th>Training Tokens -> </tg>
					 {% for token in vocabulary %}
						 {% if loop.last %}
						  <th> . . . . </th>
						 {% else %}
						 	<th>{{token}}</th>
						 {% endif %}
			 	 	 {% endfor %}
				 </tr>
			 </thead>
			 <tbody>
					 <tr>
						 <td>New Token Counts -> </td>
						 {% for count in bot %}
							 {% if loop.last %}
							   <td> . . . . </td>
							 {% else %}
						 	   <td>{{count}}</td>
							 {% endif %}
						 {% endfor %}
					 </tr>
			 </tbody>
		</table>
		<br><br>
		<a href="https://github.com/BigBangData/SMS_SpamDetect/blob/74ca298ef7cffccca1350f852766b2401d7a4e91/custom/deploy_models.py#L89">Deployment Code</a>,
		<a href="https://github.com/BigBangData/SMS_SpamDetect/blob/74ca298ef7cffccca1350f852766b2401d7a4e91/custom/clean_preprocess.py#L278">WordCounterToVectorTransformer Code</a>
	</p>
	</p>
	<p>
		<br>
		<h3>Changing the DTM into a Document-Term TF-IDF Matrix</h3>
		<br><br>
		As you can imagine, for a single document, the vector of counts consists of mostly 0s since each text has a minuscule fraction of all tokens in all texts.
		This means the information signal in a given vector is very faint. To boost the signal...
		<br><br>
		IN PROGRESS...
		<br><br>
	</p>
	<p>
		<br>
		<h3>Reducing Dimensions via Singular Value Decomposition</h3>
		<br><br>
		IN PROGESS...
		<br><br>
	</p>
	<p>
		<br>
		<h3>Computing Mean Spam Cosine Similarities</h3>
		<br><br>
		IN PROGESS...
		<br><br>
	</p>
	<p>
	<br>
	<h2 style="color:#3c90b5;">Feeding The Processed Text Into A Model</h2>
	<br>
	</p>
	<p>
	<br>
	<h2 style="color:#3c90b5;">Making A Prediction</h2>
	<br>
	</p>
	<p>
	<br>
	<h2 style="color:#3c90b5;">Rendering The Prediction</h2>
	<br>
	</p>
	<script>
	function ShowDeets1() {
	  var x = document.getElementById("Deets1");
	  if (x.style.display === "block") {
	    x.style.display = "none";
	  } else {
	    x.style.display = "block";
	  }
	}
	function ShowDeets2() {
		var y = document.getElementById("Deets2");
	  if (y.style.display === "block") {
	    y.style.display = "none";
	  } else {
	    y.style.display = "block";
	  }
	}
	function ShowDeets3() {
		var y = document.getElementById("Deets3");
	  if (y.style.display === "block") {
	    y.style.display = "none";
	  } else {
	    y.style.display = "block";
	  }
	}
	</script>
</body>
</html>
<footer>
	<div>
		<ul>
			<h3><a href="/">Try Again!</a></h3>
		</ul>
	</div>
</footer>
